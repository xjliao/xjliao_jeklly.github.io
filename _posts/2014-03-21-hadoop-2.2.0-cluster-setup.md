---
layout: post
title: 'Centos 6.5 hadoop 2.2.0 全分布式安装'
categories:
- linux
- hadoop
tags:
- linux
- hadoop

---

hadoop 2.2.0 cluster setup
=

环境：

操作系统:Centos 6.5  
jdk:jdk1.7.0_51  
hadoop版本:2.2.0  

>hostname        ip

>master     192.168.1.180

>slave1     192.168.1.181

>slave2     192.168.1.182

>slave3     192.168.1.183

## 一、前期系统环境配置
1.设置主机名

```console
$ hostname master //临时设置，重启无效
$ vim /etc/sysconfig/network //永久设置,需要重启

NETWORKING=yes
HOSTNAME=master
```
2.设置ip

```console
$ vim /etc/sysconfig/network-scripts/ifcfg-eth0

DEVICE=eth0
TYPE=Ethernet
UUID=390e4922-0d95-4c34-9dde-897fb8acef0f
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=none
HWADDR=08:00:27:1C:57:24
IPADDR=192.168.1.180
PREFIX=24
GATEWAY=192.168.1.1
DNS1=192.168.1.1
DOMAIN=114.114.114.114
DEFROUTE=yes
IPV4_FAILURE_FATAL=yes
IPV6INIT=no
NAME="System eth0"
LAST_CONNECT=1393996666
```

3.host 映射

```console
$ vim /etc/hosts

127.0.0.1   localhost
192.168.1.180 master localhost
192.168.1.181 slave1
192.168.1.182 slave2
192.168.1.183 slave3
```

4.关闭防火墙

```console
$ service iptables stop //停止防火墙
$ chkconfig iptables off  //开机启动关闭
```

5.ssh 无密码登录，master 需要启动slave节点的守护进程


## 二、hadoop 配置文件
主要需修改的配置文件  
我们先在master机器上修改这些配置文件，然后传到各个节点
hadoop 目录在~/hadoop-2.2.0

>hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-env.sh、yarn-site.xml、masters、slaves

以上配置文件，如果没有则自己新建 

## 三、master 机器配置
1.hadoop-env.sh 修改JAVA_HOME

```console
$ cd ~/hadoop-2.2.0/etc/hadoop
$ vim hadoop-env.sh
```

```sh
# The only required environment variable is JAVA_HOME.  All others are
# optional.  When running a distributed configuration it is best to
# set JAVA_HOME in this file, so that it is correctly defined on
# remote nodes.

# The java implementation to use.
export JAVA_HOME=/home/lxj/jdk-1.7.0_15
```

2.core-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
       <name>fs.default.name</name>
       <value>hdfs://master:9000</value>
    </property>
    <property>
       <name>hadoop.tmp.dir</name>
       <value>/home/lxj/hadoop/tmp</value>
    </property>
</configuration>
```

3.hdfs-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/home/lxj/hadoop/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/home/lxj/hadoop/hdfs/datanode</value>
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
</configuration>
```

4.mapred-site.xml

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>master:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>master:19888</value>
    </property>
</configuration>
```

5.yarn-env.sh 修改JAVA_HOME

```sh
# some Java parameters
export JAVA_HOME=/home/lxj/jdk1.7.0_51
if [ "$JAVA_HOME" != "" ]; then
  #echo "run java in $JAVA_HOME"
  JAVA_HOME=$JAVA_HOME
fi
```

6.yarn-site.xml

```xml
<?xml version="1.0"?>

<configuration>
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
        <property>
            <name>yarn.nodemanager.aux-servicex.mapreduce.shuffle.class</name>
            <value>org.apache.hadoop.mapred.ShuffleHandler</value>
        </property>
        <property>
            <name>yarn.resourcemanager.address</name>
            <value>master:8032</value>
        </property>
        <property>
            <name>yarn.resourcemanager.scheduler.address</name>
            <value>master:8030</value>
        </property>
        <property>
            <name>yarn.resourcemanager.resource-tracker.address</name>
            <value>master:8031</value>
        </property>
        <property>
            <name>yarn.resourcemanager.admin.address</name>
            <value>master:8033</value>
        </property>
        <property>
            <name>yarn.resourcemanager.webapp.address</name>
            <value>maseter:8088</value>
        </property>
</configuration>
```                                

7.masters 添加master主机

```sh
master  //多个换行写
```
8.slaves 添加slave节点

```sh
slave1  //当写master时，master即当作master又当作slave节点
slave2
slave3
```

## 四、slave 节点配置
slave节点的hadoop路径需和master的路径一样，当然系统环境需要保持一致。  
只要将master主机的配置好的文件拷贝到各个节点即可。

```console
$ scp -r ~/hadoop-2.2.0 lxj@slave1:/home/lxj/
```
其他节点个发送一份！


## 五、 hadoop 启动


